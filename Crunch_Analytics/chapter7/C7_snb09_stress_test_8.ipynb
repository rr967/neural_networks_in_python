{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stress test 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the 3 functions we defined from the model building notebook and new one to split data\n",
    "from model_functions_3 import load_and_preprocess_data, make_churn_model, test_model, import_data_and_split_datasets\n",
    "\n",
    "#train model on june data\n",
    "rf, acc, conf_matrix = make_churn_model('data_jun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the model on new data from july\n",
    "acc, conf_matrix = test_model('data_jul_broken8', rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at feature importance of model in june and july and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "X, y = load_and_preprocess_data('data_jul_broken8')\n",
    "\n",
    "# Split the data into test and training (30% for test)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify = y,\n",
    "                                                    random_state = 123)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the classifier using the train data\n",
    "rf = rf.fit(X_train, Y_train)\n",
    "\n",
    "# Validate the classifier\n",
    "accuracy = rf.score(X_test, Y_test)\n",
    "\n",
    "# Make a confusion matrix\n",
    "prediction = rf.predict(X_test)\n",
    "\n",
    "conf_matrix = pd.DataFrame(\n",
    "    confusion_matrix(Y_test, prediction), \n",
    "    columns=[\"Predicted False\", \"Predicted True\"], \n",
    "    index=[\"Actual False\", \"Actual True\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use this library to make the display pretty\n",
    "# install this module using: \n",
    "# $ conda install tabulate\n",
    "from tabulate import tabulate\n",
    "\n",
    "headers = [\"name\", \"score\"]\n",
    "values = sorted(zip(X_train.columns, rf.feature_importances_), key=lambda x: x[1] * -1)\n",
    "print(tabulate(values, headers, tablefmt=\"plain\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "X, y = load_and_preprocess_data('data_jul')\n",
    "\n",
    "# Split the data into test and training (25% for test)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.25,\n",
    "                                                    stratify = y,\n",
    "                                                    random_state = 123)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Train the classifier using the train data\n",
    "rf = rf.fit(X_train, Y_train)\n",
    "\n",
    "# Validate the classifier\n",
    "accuracy = rf.score(X_test, Y_test)\n",
    "\n",
    "# Make a confusion matrix\n",
    "prediction = rf.predict(X_test)\n",
    "\n",
    "conf_matrix = pd.DataFrame(\n",
    "    confusion_matrix(Y_test, prediction), \n",
    "    columns=[\"Predicted False\", \"Predicted True\"], \n",
    "    index=[\"Actual False\", \"Actual True\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use this library to make the display pretty\n",
    "# install this module using: \n",
    "# $ conda install tabulate\n",
    "from tabulate import tabulate\n",
    "\n",
    "headers = [\"name\", \"score\"]\n",
    "values = sorted(zip(X_train.columns, rf.feature_importances_), key=lambda x: x[1] * -1)\n",
    "print(tabulate(values, headers, tablefmt=\"plain\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the predictive power of app_usage has diminished in the july model. Has app_usage become a bad predictor of data? Or is the data not correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a check to see if top 3 predictors is the same across data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Check 8:\n",
    "Check if feature importance is the same across datasets\n",
    "If not: return false\n",
    "'''\n",
    "from itertools import compress\n",
    "def check_feature_importance(csv_name_old, csv_name):\n",
    "    \n",
    "    #load data\n",
    "    X_old, y_old = load_and_preprocess_data(csv_name_old)\n",
    "    X, y = load_and_preprocess_data(csv_name)\n",
    "    \n",
    "    \n",
    "    #Make model for old data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_old, \n",
    "                                                        y_old, \n",
    "                                                        test_size=0.25,\n",
    "                                                        stratify = y_old,\n",
    "                                                        random_state = 123)\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "    # Train the classifier using the train data\n",
    "    rf = rf.fit(X_train, Y_train)\n",
    "    \n",
    "    feat_importance = pd.Series(rf.feature_importances_)\n",
    "    var = pd.Series(range(len(feat_importance)))\n",
    "    old_feat_importance = pd.concat([feat_importance, var], axis=1)\n",
    "    old_feat_importance.columns = ['feat_importance', 'var']\n",
    "    old_feat_importance = old_feat_importance.sort_values(by = 'feat_importance', ascending=False)\n",
    "    old_feat_importance = old_feat_importance[0:3]\n",
    "\n",
    "    \n",
    "    #Make model for new data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size=0.25,\n",
    "                                                        stratify = y,\n",
    "                                                        random_state = 123)\n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "    # Train the classifier using the train data\n",
    "    rf = rf.fit(X_train, Y_train)\n",
    "    \n",
    "    feat_importance = pd.Series(rf.feature_importances_)\n",
    "    var = pd.Series(range(len(feat_importance)))\n",
    "    new_feat_importance = pd.concat([feat_importance, var], axis=1)\n",
    "    new_feat_importance.columns = ['feat_importance', 'var']\n",
    "    new_feat_importance = new_feat_importance.sort_values(by = 'feat_importance', ascending=False)\n",
    "    new_feat_importance = new_feat_importance[0:3]\n",
    "    \n",
    "    \n",
    "    #compare feature importance\n",
    "    ft1 = old_feat_importance['var'].values > new_feat_importance['var'].values\n",
    "    ft2 = old_feat_importance['var'].values < new_feat_importance['var'].values\n",
    "    \n",
    "    #if we add ft1 and ft2 we will always get 1 if they do not match\n",
    "    ft = ft1 + ft2\n",
    "    \n",
    "    #if any value is true, output true\n",
    "    answer = ft.any()\n",
    "    \n",
    "    #toggle true to false to get right answer\n",
    "    def toggle_true_false(x):\n",
    "        return not x\n",
    "    answer = toggle_true_false(answer)\n",
    "        \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name_old = 'data_jun'\n",
    "csv_name = 'data_jul_broken8'\n",
    "check_feature_importance(csv_name_old, csv_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
<<<<<<< Updated upstream
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
=======
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
