{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying data science models and stress testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We are going to create a churn model for a SaaS vendor who wants to see which of his/her clients are likely to churn. To do this you have been provided with data in the file **data_jun.csv** (in the data directory). \n",
    "\n",
    "This dataset contains observations from the last 6 months for the following variables:\n",
    "\n",
    "1. **churn** has the customer churned (1 = yes)? (you may assume that the necessary time shift to make this variable relevant has already happened.) \n",
    "2. **app_usage** how much has he/she used the app? \n",
    "3. **payment_method** how does this customer make payments?\n",
    "4. **payment_freq** frequency of payments\n",
    "5. **location** country of the client\n",
    "6. **platform** the platform the customer uses\n",
    "7. **userId** the unique identifier\n",
    "8. **month** the month during which the observation was tracked\n",
    "9. **sub_amount** the subscription amount paid by the user\n",
    "\n",
    "## Briefing\n",
    "\n",
    "Your first goal is to create a churn model based on this data using the techniques you've seen in the past weeks. Next we will be testing this model on the \"updated\" dataset you receive in the next month, as you might suspect ther might be some data issues. \n",
    "\n",
    "Once the model has been created, you model will be transferred into .py files rather than notebooks that can be called for scripts by execution (we have provided you with example source files but please refrain from looking at these for now). \n",
    "\n",
    "## Footnotes\n",
    "\n",
    "You may regard each of these records as 'individual' observations, which is an oversimplification of the true nature of a churn problem since these observations are of course not independent - and in an ideal scenario a churn model includes quite a lot more complex analysis (offsets,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the most recent data\n",
    "app_df = pd.read_csv('data/data_jun.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "app_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
<<<<<<< Updated upstream
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
=======
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
