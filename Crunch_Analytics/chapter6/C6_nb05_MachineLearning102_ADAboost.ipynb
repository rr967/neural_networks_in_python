{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5 Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 6.5.1 Some notes on Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Adaboost is a very popular (and simple) example of an ensemble learning technique. \n",
    "\n",
    "The basic principle behind the Adaboost technique is a combination of weak classifiers (of the same type) in order to create a good over-arching classifier. A typical application of this technique is the use of very small decision trees (aka decision stumps) into a larger and more complex classification system. \n",
    "\n",
    "A key aspect of this approach is that the models are created in a **sequential** fashion, actively trying to find new classifiers that can improve on areas where the current ensemble method is performing badly. Due to this behaviour it is a technique that is very prone to overfitting, hence it is important to avoid the following: \n",
    "\n",
    "* **Noise in the dependent variable**: if there are errors in the Y variable, it is highly likely that these will be picked up by one or more weak estimators, so these have to be avoided at all cost. \n",
    "* **Outliers**: if the dataset you are using contains very specific outliers these are best removed prior to using an Adaboost technique. If not it is very likely that the Adaboost technique will construct a far-fetched logic just to get the outliers correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 6.5.2 Import dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To illustrate Adaboost we are going to try and classify mushrooms, the dataset used for this problem was downloaded from Kaggle (https://www.kaggle.com/uciml/mushroom-classification/data)\n",
    "\n",
    "<img src=\"figures/mushroom.jpg\" alt=\"mushroom\" style=\"width: 35%;\"/>\n",
    "\n",
    "The dependent variable is 'class', defined as: \n",
    "\n",
    "* **e** for edible\n",
    "* **p** for poisonous\n",
    "\n",
    "The following independent variables are available to make predictions: \n",
    "\n",
    "* **cap-shape**: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n",
    "* **cap-surface**: fibrous=f,grooves=g,scaly=y,smooth=s\n",
    "* **cap-color**: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "* **bruises**: bruises=t,no=f\n",
    "* **odor**: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n",
    "* **gill-attachment**: attached=a,descending=d,free=f,notched=n\n",
    "* **gill-spacing**: close=c,crowded=w,distant=d\n",
    "* **gill-size**: broad=b,narrow=n\n",
    "* **gill-color**: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "* **stalk-shape**: enlarging=e,tapering=t\n",
    "* **stalk-root**: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n",
    "* **stalk-surface-above-ring**: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "* **stalk-surface-below-ring**: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "* **stalk-color-above-ring**: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "* **stalk-color-below-ring**: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "* **veil-type**: partial=p,universal=u\n",
    "* **veil-color**: brown=n,orange=o,white=w,yellow=y\n",
    "* **ring-number**: none=n,one=o,two=t\n",
    "* **ring-type**: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n",
    "* **spore-print-color**: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n",
    "* **population**: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n",
    "* **habitat**: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mushrooms_df = pd.read_csv('data/mushrooms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To make the problem a bit harder we are throw away some data (heresy!)\n",
    "mushrooms_df = mushrooms_df[mushrooms_df.columns[:5]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 6.5.3 Explore the basic properties of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will work with a reduced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mushrooms_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Check if the problem is not massively skewed\n",
    "sns.countplot('class', data=mushrooms_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# The character encoded labels are difficult to work with\n",
    "# So we are going to convert them to integer labels\n",
    "factored_mapping = {}\n",
    "mushrooms_fac_df = pd.DataFrame()\n",
    "for c in mushrooms_df.columns[1:]:\n",
    "    labels, levels = pd.factorize(mushrooms_df[c])\n",
    "    mushrooms_fac_df[c] = pd.Series(labels)\n",
    "    factored_mapping[c] = levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# I'm going to explicitly map the dependent variable to make sure\n",
    "# that my interpretation is correct \n",
    "mush_map = {}\n",
    "mush_map['e'] = 0\n",
    "mush_map['p'] = 1\n",
    "mushrooms_fac_df['class'] = mushrooms_df['class'].map(mush_map).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mushrooms_fac_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Look at the correlations of the different categorical variables\n",
    "sns.heatmap(mushrooms_fac_df.corr(method='kendall'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 6.5.4 Prepare the dataset for learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are going to do one-hot encoding on this dataset to make sure that it can be used with all types of models. This will make all variables, regardless of type, binary. Note that this assumes that we are not going to use techniques where the dummy variable trap (only n-1 dummy's needed!) might apply, such as simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mush_X = ohe.fit_transform(mushrooms_fac_df.drop(['class'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is outputted as a sparse matrix since there might be a lot of zeros! \n",
    "type(mush_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mush_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Encode the dependent as a binary variable\n",
    "mush_Y = mushrooms_fac_df['class'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mush_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(mush_X, mush_Y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 6.5.5 Find the right parameters for the Adaboost model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Because the Adaboost model is prone to overfitting, it is of paramount importance to use a good test design to implement this model. Ideally this is done using e K-fold cross validation test design, as you have been shown in previous notebooks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Prepare an empty paramter grid for the Adaboost Model\n",
    "p_grid = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 6.5.5.1 Parameter: The base estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The Adaboost uses a combination of simple estimators, the most essential parameter to select for this model is the **type of base estimator** to be used. Ideally the complexity of these base estimators is limited. \n",
    "\n",
    "An important notion in this respect is that **these simple estimators have parameters of their own!** This immediately implies that these parameters must also be tuned! (parameter-ception!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p_grid['base_estimator'] = [DecisionTreeClassifier(max_depth=1), # This is a true 'stump'\n",
    "                            DecisionTreeClassifier(max_depth=2)] # This allows for a little bit extra complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 6.5.5.2 Parameter: The number of estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first key parameter for the Adaboost model is the number of estimators. These is the maximum number of 'simple' estimators that can be combined in order to feed the ensemble model. Using a higher number will generally increase performance, but also greatly increases the risk of overfitting the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p_grid['n_estimators'] = [10, 25, 50, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 6.5.5.3 Parameter: The learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The learning rate specifies how much the influence of additional estimators is decreased. This parameter can be important to limit the degree of overfitting of a model. \n",
    "\n",
    "The learning itself is a value in the range $]0, 1]$, where a smaller learning rate means that each additional estimator has less impact. \n",
    "\n",
    "It is evident that there is a trade-off between the value of the learning rate and the number of estimators, when the learning rate is decreased a larger number of estimators will be required to get a model to an identical level of performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "p_grid['learning_rate'] = [1, 0.5, 0.1, 0.01]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### 6.5.5.4 Running the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# To ensure reproducibility we are going to use a single random seed\n",
    "p_grid['random_state'] = [42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier()\n",
    "grid_search = GridSearchCV(abc, \n",
    "                           param_grid=p_grid, \n",
    "                           refit=True,\n",
    "                           n_jobs=-1) # this makes sure your system uses all threads when fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Training the model using cross-validation - this might take some time depending on your system\n",
    "grid_search.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid_search.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sum(grid_search.predict(X_test) != Y_test) #This is the number of wrong predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's investigate the confusion matrix for this problem\n",
    "prediction = grid_search.predict(X_test)\n",
    "conf_matrix = pd.DataFrame(\n",
    "    confusion_matrix(Y_test, prediction), \n",
    "    columns=[\"Predicted False\", \"Predicted True\"], \n",
    "    index=[\"Actual False\", \"Actual True\"]\n",
    ")\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 6.5.6 Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Minimize the risk of eating poisonous mushrooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Looking at the confusion matrix you can see that there are still a number of mushrooms that incorrectly classified, this is of course a quite substantial risk. \n",
    "\n",
    "Is it possible to set the 'threshold' which you are using at such a position that it practically guarantees that all the mushrooms that are classified as edible are not poisonous? \n",
    "\n",
    "The following function should come in handy:\n",
    "```\n",
    "grid_search.predict_proba\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Improve the classifier's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The code above only used one kind of kernel to make the prediction (the estimator parameter), however mutliple other kernels can be used and could potentially improve performance. Test and see if you can find one that further improves the performance of the model. \n",
    "\n",
    "The following estimators can be used: \n",
    "\n",
    "* BernoulliNB\n",
    "* DecisionTreeClassifier\n",
    "* ExtraTreeClassifier\n",
    "* ExtraTreesClassifier\n",
    "* MultinomialNB\n",
    "* NuSVC\n",
    "* Perceptron\n",
    "* RandomForestClassifier\n",
    "* RidgeClassifierCV\n",
    "* SGDClassifier\n",
    "* SVC\n",
    "\n",
    "To find more information on these estimators you can look up their documentation at http://scikit-learn.org/\n",
    "\n",
    "HINT. If you run across an error, try algorithm='SAMME' in stead of algorithm='SAMME.R'.\n",
    "\n",
    "HINT2. Each classifier uses different parameters, google is your friend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
